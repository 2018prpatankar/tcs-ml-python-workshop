{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import re\n",
    "import math #for logarithm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingfile='tweets.train.clean.txt'\n",
    "testfile='tweets.test1.clean.txt'\n",
    "outputfile='output.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wordlist):\n",
    "    pwordlist=wordlist\n",
    "    plowerwordlist=[]\n",
    "    for word in pwordlist:\n",
    "        word0=re.sub('[#,*,?,!,(,),&,.,:,;,/,-]', '', word)#https://stackoverflow.com/questions/13437114/how-do-i-replace-a-character-in-a-string-with-another-character-in-python\n",
    "        word1=re.sub('[_,\",\"]', ' ', word0)\n",
    "        plowerwordlist.append(word1.lower())\n",
    "    #excludelist=[]\n",
    "    excludelist=[' ','@','at','the','in','to','you','as','i','my','me','this','that','those','here','there','a','is','was','but','on','of','and','for']\n",
    "    for excludeword in excludelist:\n",
    "        if excludeword in plowerwordlist:\n",
    "            plowerwordlist.remove(excludeword)\n",
    "    return plowerwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findclasses():#preporcessing not required for extracting the cities/classes from the training file\n",
    "    citylist=[] #list of cities or classes, city may be repeated in this specific list\n",
    "    totalnumtweets=0\n",
    "    #trngfile=open(trainingfile,'r') # https://stackoverflow.com/questions/23917729/switching-to-python-3-causing-unicodedecodeerror\n",
    "    trngf=open(trainingfile,'r') #Entire Training Document and names of Classess i.e. Entire training tweets and Cities\n",
    "    ctraintweets=[]\n",
    "    totalnumtweets=0 # Total numer of training documents or tweets \n",
    "    for ctweet in trngf:\n",
    "        ctraintweets.append(ctweet)\n",
    "        words=ctweet.split()  \n",
    "        citylist.append(words[0])\n",
    "        totalnumtweets+=1\n",
    "    cityseries=pd.Series(citylist)\n",
    "    citycount=cityseries.value_counts()\n",
    "    #print(totalnumtweets,listofcities) # 32K tweets; 12 cities\n",
    "    trngf.close()\n",
    "    return(citycount,totalnumtweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(citycount,N):        \n",
    "    trngf1=open(trainingfile,'r') #Entire Training Document and names of Classess i.e. Entire training tweets and Cities\n",
    "    ctraintweets=[]\n",
    "    for ctweet in trngf1:\n",
    "        ctraintweets.append(ctweet)\n",
    "    trngf1.close()\n",
    "     \n",
    "    #Building city-wise word distribution\n",
    "    cityworddist={}\n",
    "    for city in citycount.index:\n",
    "        cityvocabulary=[]\n",
    "        for ctweet in ctraintweets:\n",
    "            ctweetsp=ctweet.split()\n",
    "            if city==ctweetsp[0]:\n",
    "                cityvocabulary+=preprocess(ctweetsp)#class name also included in training; classname is an associated word!; classname not considered while evaluating test file \n",
    "        cityvocabseries=pd.Series(cityvocabulary)\n",
    "        cityvocabwordcount=cityvocabseries.value_counts()\n",
    "        cityworddist[city]=cityvocabwordcount\n",
    "        #print('******************')\n",
    "        #print(city,cityvocabulary,cityvocabseries.value_counts())\n",
    "        #print('******************')\n",
    "    #print(cityworddist)\n",
    "    \n",
    "    prior={} #Prior Probability\n",
    "    for city in citycount.index:\n",
    "        prior[city]=citycount[city]/N #P(city)        \n",
    "    return prior,cityworddist   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numofwordintraining(): #preprocessing used at the end of this function!!!\n",
    "    trngf1=open(trainingfile,'r') #Entire Training Document and names of Classess i.e. Entire training tweets and Cities\n",
    "    ctraintweets=[]\n",
    "    for ctweet in trngf1:\n",
    "        ctraintweets.append(ctweet)\n",
    "    trngf1.close()\n",
    "    num=0\n",
    "    vocab=[]\n",
    "    for ctweet in ctraintweets:\n",
    "        for word in ctweet.split()[1:]:\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "                num+=1    \n",
    "    return preprocess(vocab),num #vocab has only distinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processtestfile(citycount,prior,cityworddist,vocabsize,vocab):                    \n",
    "    trngf3=open(testfile,'r') #Entire Training Document and names of Classess i.e. Entire training tweets and Cities\n",
    "    testf=open(outputfile,'w')\n",
    "    ctesttweets=[]\n",
    "    for ctweet in trngf3:\n",
    "        ctesttweets.append(ctweet)\n",
    "    trngf3.close()\n",
    "    TP=0\n",
    "    numoftesttweets=0\n",
    "    for ctweet in ctesttweets:\n",
    "        numoftesttweets+=1\n",
    "        scores={}\n",
    "        z=0\n",
    "        for city in citycount.index:\n",
    "            score=math.log10(prior[city])\n",
    "            #print(city,prior[city])\n",
    "            #for word in preprocess(ctweet.split()):#preprocessing the test tweet; class name is also made lower case\n",
    "            for word in preprocess(ctweet.split()[1:]):#preprocessing the test tweet; class name is excluded    \n",
    "                if z==0:\n",
    "                    actualcity=ctweet.split()[0] \n",
    "                    z+=1\n",
    "                if word in cityworddist[city].index: \n",
    "                    a1=cityworddist[city][word]\n",
    "                else:\n",
    "                    a1=0\n",
    "                a2=1\n",
    "                a3=cityworddist[city].sum()\n",
    "                a4=vocabsize\n",
    "                #print(city,word,'a4',a4)\n",
    "                score+=math.log10(a1+a2)-math.log10(a3+a4) #similar results obtained by using multiplication and without logarithms\n",
    "                #if word not in vocab:\n",
    "                    #score-=math.log10(prior[city])/len(ctweet.split()[1:]) #This is to adjust the prior probability for words not in the entire training set...this does not help if the test set distribution\n",
    "                    #mirrors the distribution in the training set. However, this could help otherwise.\n",
    "            scores[city]=score\n",
    "            #print(city,scores[city])\n",
    "        #print('*************')\n",
    "        #print(scores)\n",
    "        #print('*************')\n",
    "        v=list(scores.values())\n",
    "        k=list(scores.keys())\n",
    "        predcity=k[v.index(max(v))]\n",
    "        predv=max(v)\n",
    "        #print(predcity,actualcity,predv,ctweet) #see this for tweet level predictions\n",
    "        testf.write(predcity+' '+ctweet) \n",
    "        if predcity.lower()==actualcity.lower():\n",
    "            TP+=1\n",
    "    accuracy=100*TP/numoftesttweets\n",
    "    print('True Positives=',TP,'Accuracy=',accuracy,'%')\n",
    "    #testf.write('TP='+str(TP)+' Accuracy='+str(accuracy)+'%') #syntax needs to be changed after importing re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citytopwords(citycount,cityworddist,n):\n",
    "    print('Top words asscociated with each city:')\n",
    "    for city in citycount.index:\n",
    "        print(city,'\\n',cityworddist[city][:n])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives= 320 Accuracy= 64.0 %\n",
      "Top words asscociated with each city:\n",
      "Manhattan,_NY \n",
      " manhattan ny    6787\n",
      "new             1873\n",
      "york            1671\n",
      "ny              1279\n",
      "                1207\n",
      "dtype: int64\n",
      "Los_Angeles,_CA \n",
      " los angeles ca    5988\n",
      "                  1155\n",
      "ca                 998\n",
      "los                802\n",
      "job                791\n",
      "dtype: int64\n",
      "Chicago,_IL \n",
      " chicago il    3020\n",
      "chicago       1635\n",
      "il             927\n",
      "               636\n",
      "job            519\n",
      "dtype: int64\n",
      "Houston,_TX \n",
      " houston tx    2461\n",
      "houston       1306\n",
      "tx             967\n",
      "job            764\n",
      "               755\n",
      "dtype: int64\n",
      "San_Francisco,_CA \n",
      " san francisco ca    2103\n",
      "                     573\n",
      "ca                   558\n",
      "san                  500\n",
      "sanfrancisco         465\n",
      "dtype: int64\n",
      "Atlanta,_GA \n",
      " atlanta ga    1910\n",
      "atlanta        726\n",
      "               451\n",
      "ga             423\n",
      "job            328\n",
      "dtype: int64\n",
      "Toronto,_Ontario \n",
      " toronto ontario    1890\n",
      "toronto             676\n",
      "                    604\n",
      "job                 210\n",
      "hiring              163\n",
      "dtype: int64\n",
      "Washington,_DC \n",
      " washington dc    1744\n",
      "dc                615\n",
      "washington        600\n",
      "                  374\n",
      "job               270\n",
      "dtype: int64\n",
      "Philadelphia,_PA \n",
      " philadelphia pa    1663\n",
      "philadelphia        576\n",
      "pa                  373\n",
      "                    336\n",
      "job                 187\n",
      "dtype: int64\n",
      "San_Diego,_CA \n",
      " san diego ca    1568\n",
      "ca               501\n",
      "                 429\n",
      "san              370\n",
      "sandiego         366\n",
      "dtype: int64\n",
      "Boston,_MA \n",
      " boston ma    1465\n",
      "boston        762\n",
      "ma            445\n",
      "job           382\n",
      "              360\n",
      "dtype: int64\n",
      "Orlando,_FL \n",
      " orlando fl    1401\n",
      "orlpol         514\n",
      "orlando        496\n",
      "opd            445\n",
      "fl             267\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ## from the training file\n",
    "    citycount,N=findclasses()  ## total number of cities and total number of tweets...\n",
    "    vocab,vocabsize=numofwordintraining() ## list of distinct words, total number of distinct words\n",
    "    #print(citycount,N)\n",
    "    prior,cityworddist=train(citycount,N) ## prior probabilities, city-wise word distribution\n",
    "    \n",
    "    ## from the test file\n",
    "    processtestfile(citycount,prior,cityworddist,vocabsize,vocab) ## Bayesian Inference\n",
    "    citytopwords(citycount,cityworddist,5)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
